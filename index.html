<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models">
  <meta property="og:title" content="SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models"/>
  <meta property="og:description" content="A benchmark that pairs semantically equivalent inputs across four domains with existing standardized textual and visual notations."/>
  <meta property="og:url" content="https://csslab.github.io/SEAM/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/heatmap.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models">
  <meta name="twitter:description" content="A benchmark that pairs semantically equivalent inputs across four domains with existing standardized textual and visual notations.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/heatmap.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="vision-language models, multimodal, benchmark, evaluation, semantic equivalence, VLM, machine learning, artificial intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SEAM: Semantically Equivalent Across Modalities Benchmark</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><strong>SEAM</strong>: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lilv98.github.io/" target="_blank">Zhenwei Tang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/difan-jiao/?originalSubdomain=ca" target="_blank">Difan Jiao</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.toronto.edu/~blair/" target="_blank">Blair Yang</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.toronto.edu/~ashton/" target="_blank">Ashton Anderson</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup><a href="https://csslab.cs.toronto.edu/" target="_blank">CSSLab</a>, Department of Computer Science, University of Toronto<br><sup>2</sup>Coolwei AI Lab<br>[COLM '25] Second Conference on Language Modeling</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- OpenReview PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=lI4LgGv4sX" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- HuggingFace Dataset link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/lilvjosephtang/SEAM-Benchmark" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/CSSLab/SEAM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- arXiv Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2508.18179" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- OpenReview abstract Link -->
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=lI4LgGv4sX" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-comments"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified is-size-5">
          <p>
            Evaluating whether vision–language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce <strong>SEAM</strong>, a benchmark that pairs semantically equivalent inputs across four domains with existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notations and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Results Images -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Overview</h2>
      
      <!-- Main Results Figure -->
      <div class="columns is-centered" style="margin-bottom: 3rem;">
        <div class="column is-four-fifths has-text-centered">
          <img src="static/images/main.png" alt="Main results showing SEAM benchmark performance" style="max-width: 100%; height: auto;"/>
          <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
            <strong>SEAM</strong> includes 16 tasks in chess, chemistry, music, and graph theory domains with paired visual-spatial and textual-symbolic representations that are semantically equivalent.
          </h3>
        </div>
      </div>

    </div>
  </div>
</section>


<!-- Leaderboard -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Leaderboard</h2>
      <div class="columns is-centered">
        <div class="column">
          <p class="has-text-centered" style="margin-bottom: 1.5rem;">
            Interactive leaderboard of VLMs across language (L), vision (V), and vision-language (VL) modalities. 
            <strong>Click column headers to sort.</strong> Models are initially sorted by L-V agreement.
          </p>
          
          <div class="table-container" style="overflow-x: auto;">
            <table id="leaderboard" class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th class="sortable" data-column="0">Model</th>
                  <th class="sortable" data-column="1">L Accuracy</th>
                  <th class="sortable" data-column="2">V Accuracy</th>
                  <th class="sortable" data-column="3">VL Accuracy</th>
                  <th class="sortable" data-column="4">Avg Accuracy</th>
                  <th class="sortable" data-column="5" style="background-color: #f5f5f5;"><strong>L-V Agreement</strong></th>
                  <th class="sortable" data-column="6">L-VL Agreement</th>
                  <th class="sortable" data-column="7">V-VL Agreement</th>
                  <th class="sortable" data-column="8">All Agreement</th>
                </tr>
              </thead>
              <tbody id="leaderboard-body">
                <!-- Proprietary Models -->
                <tr class="proprietary-header">
                  <td colspan="9" style="font-style: italic; font-weight: bold; background-color: #e8e8e8;">Proprietary Models</td>
                </tr>
                <tr data-category="proprietary">
                  <td>GPT-5-mini</td>
                  <td>0.787</td>
                  <td class="best">0.653</td>
                  <td class="second-best">0.830</td>
                  <td class="second-best">0.756</td>
                  <td class="best" style="background-color: #f5f5f5;">0.630</td>
                  <td class="second-best">0.846</td>
                  <td class="second-best">0.653</td>
                  <td class="second-best">0.584</td>
                </tr>
                <tr data-category="proprietary">
                  <td>GPT-5</td>
                  <td>0.804</td>
                  <td class="second-best">0.632</td>
                  <td class="best">0.857</td>
                  <td class="best">0.765</td>
                  <td class="second-best" style="background-color: #f5f5f5;">0.627</td>
                  <td class="best">0.876</td>
                  <td class="best">0.657</td>
                  <td class="best">0.596</td>
                </tr>
                <tr data-category="proprietary">
                  <td>Claude-3.7-Sonnet</td>
                  <td>0.743</td>
                  <td>0.591</td>
                  <td>0.679</td>
                  <td>0.671</td>
                  <td style="background-color: #f5f5f5;">0.594</td>
                  <td>0.715</td>
                  <td>0.624</td>
                  <td>0.506</td>
                </tr>
                <tr data-category="proprietary">
                  <td>Claude-4.1-Opus</td>
                  <td class="best">0.827</td>
                  <td>0.578</td>
                  <td>0.814</td>
                  <td>0.740</td>
                  <td style="background-color: #f5f5f5;">0.575</td>
                  <td>0.844</td>
                  <td>0.580</td>
                  <td>0.523</td>
                </tr>
                <tr data-category="proprietary">
                  <td>Claude-4-Sonnet</td>
                  <td class="second-best">0.808</td>
                  <td>0.545</td>
                  <td>0.803</td>
                  <td>0.719</td>
                  <td style="background-color: #f5f5f5;">0.569</td>
                  <td>0.834</td>
                  <td>0.566</td>
                  <td>0.508</td>
                </tr>
                <tr data-category="proprietary">
                  <td>Claude-3.5-Sonnet</td>
                  <td>0.665</td>
                  <td>0.560</td>
                  <td>0.514</td>
                  <td>0.580</td>
                  <td style="background-color: #f5f5f5;">0.537</td>
                  <td>0.549</td>
                  <td>0.508</td>
                  <td>0.378</td>
                </tr>
                <tr data-category="proprietary">
                  <td>GPT-4o</td>
                  <td>0.635</td>
                  <td>0.482</td>
                  <td>0.627</td>
                  <td>0.581</td>
                  <td style="background-color: #f5f5f5;">0.503</td>
                  <td>0.686</td>
                  <td>0.532</td>
                  <td>0.410</td>
                </tr>
                <tr data-category="proprietary">
                  <td>GPT-5-nano</td>
                  <td>0.699</td>
                  <td>0.510</td>
                  <td>0.753</td>
                  <td>0.654</td>
                  <td style="background-color: #f5f5f5;">0.500</td>
                  <td>0.771</td>
                  <td>0.516</td>
                  <td>0.432</td>
                </tr>
                <tr data-category="proprietary">
                  <td>GPT-4o-mini</td>
                  <td>0.555</td>
                  <td>0.411</td>
                  <td>0.529</td>
                  <td>0.498</td>
                  <td style="background-color: #f5f5f5;">0.480</td>
                  <td>0.650</td>
                  <td>0.518</td>
                  <td>0.379</td>
                </tr>
                <tr data-category="proprietary">
                  <td>Claude-3.5-Haiku</td>
                  <td>0.530</td>
                  <td>0.433</td>
                  <td>0.496</td>
                  <td>0.486</td>
                  <td style="background-color: #f5f5f5;">0.479</td>
                  <td>0.556</td>
                  <td>0.534</td>
                  <td>0.346</td>
                </tr>
                
                <!-- Open-Source Models -->
                <tr class="opensource-header">
                  <td colspan="9" style="font-style: italic; font-weight: bold; background-color: #e8e8e8;">Open-Source Models</td>
                </tr>
                <tr data-category="opensource">
                  <td>Qwen2.5-VL-72B-Instruct</td>
                  <td class="best">0.547</td>
                  <td class="best">0.475</td>
                  <td class="best">0.519</td>
                  <td class="best">0.514</td>
                  <td class="best" style="background-color: #f5f5f5;">0.447</td>
                  <td class="best">0.504</td>
                  <td>0.532</td>
                  <td>0.318</td>
                </tr>
                <tr data-category="opensource">
                  <td>InternVL3-78B</td>
                  <td class="second-best">0.525</td>
                  <td>0.427</td>
                  <td class="second-best">0.482</td>
                  <td class="second-best">0.478</td>
                  <td class="best" style="background-color: #f5f5f5;">0.447</td>
                  <td class="second-best">0.498</td>
                  <td>0.487</td>
                  <td>0.293</td>
                </tr>
                <tr data-category="opensource">
                  <td>gemma-3-27b-it</td>
                  <td>0.516</td>
                  <td class="second-best">0.428</td>
                  <td>0.450</td>
                  <td>0.465</td>
                  <td class="best" style="background-color: #f5f5f5;">0.447</td>
                  <td>0.497</td>
                  <td class="best">0.575</td>
                  <td class="best">0.325</td>
                </tr>
                <tr data-category="opensource">
                  <td>gemma-3-12b-it</td>
                  <td>0.458</td>
                  <td>0.401</td>
                  <td>0.429</td>
                  <td>0.429</td>
                  <td class="second-best" style="background-color: #f5f5f5;">0.419</td>
                  <td>0.474</td>
                  <td class="second-best">0.543</td>
                  <td>0.297</td>
                </tr>
                <tr data-category="opensource">
                  <td>InternVL-2.5-78B</td>
                  <td>0.448</td>
                  <td>0.414</td>
                  <td>0.459</td>
                  <td>0.440</td>
                  <td style="background-color: #f5f5f5;">0.415</td>
                  <td>0.485</td>
                  <td>0.523</td>
                  <td class="second-best">0.309</td>
                </tr>
                <tr data-category="opensource">
                  <td>InternVL3-8B</td>
                  <td>0.382</td>
                  <td>0.357</td>
                  <td>0.386</td>
                  <td>0.375</td>
                  <td style="background-color: #f5f5f5;">0.388</td>
                  <td>0.425</td>
                  <td>0.456</td>
                  <td>0.229</td>
                </tr>
                <tr data-category="opensource">
                  <td>Llama-3.2-90B-Vision-Instruct</td>
                  <td>0.434</td>
                  <td>0.384</td>
                  <td>0.439</td>
                  <td>0.419</td>
                  <td style="background-color: #f5f5f5;">0.384</td>
                  <td>0.460</td>
                  <td>0.443</td>
                  <td>0.253</td>
                </tr>
                <tr data-category="opensource">
                  <td>Qwen2.5-Omni-7B</td>
                  <td>0.363</td>
                  <td>0.354</td>
                  <td>0.364</td>
                  <td>0.360</td>
                  <td style="background-color: #f5f5f5;">0.353</td>
                  <td>0.375</td>
                  <td>0.375</td>
                  <td>0.183</td>
                </tr>
                <tr data-category="opensource">
                  <td>Qwen2.5-VL-7B-Instruct</td>
                  <td>0.303</td>
                  <td>0.350</td>
                  <td>0.359</td>
                  <td>0.337</td>
                  <td style="background-color: #f5f5f5;">0.347</td>
                  <td>0.389</td>
                  <td>0.437</td>
                  <td>0.216</td>
                </tr>
                <tr data-category="opensource">
                  <td>InternVL-2.5-8B</td>
                  <td>0.324</td>
                  <td>0.337</td>
                  <td>0.334</td>
                  <td>0.332</td>
                  <td style="background-color: #f5f5f5;">0.324</td>
                  <td>0.340</td>
                  <td>0.436</td>
                  <td>0.196</td>
                </tr>
                <tr data-category="opensource">
                  <td>Llama-3.2-11B-Vision-Instruct</td>
                  <td>0.289</td>
                  <td>0.330</td>
                  <td>0.323</td>
                  <td>0.314</td>
                  <td style="background-color: #f5f5f5;">0.287</td>
                  <td>0.303</td>
                  <td>0.401</td>
                  <td>0.152</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Accuracy vs Agreement</h2>

      <!-- Accuracy vs Agreement -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <img src="static/images/acc_vs_agr.png" alt="Accuracy vs Agreement scatter plot" style="max-width: 100%; height: auto;"/>
          <h3 class="subtitle has-text-centered" style="margin-top: 1rem;">
            Correlation between final answer agreement rates with language and vision inputs and average accuracy across various models. Models are color-coded by family. Random Baseline denotes how often two models with identical accuracy would agree by random chance, which can be thought of as a lower bound for cross-modal agreement of real multimodal models. Cross-modal answer agreement is relatively low, i.e., often not far from the random baseline, suggesting that models differ substantially in how they process information across modalities, and have substantial room to improve in integrating reasoning and leveraging abilities across representations.
          </h3>
        </div>
      </div>

    </div>
  </div>
</section>
<!-- End results images -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
tang2025seam,
title={{SEAM}: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models},
author={Zhenwei Tang and Difan Jiao and Blair Yang and Ashton Anderson},
booktitle={Second Conference on Language Modeling},
year={2025},
url={https://openreview.net/forum?id=lI4LgGv4sX}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
